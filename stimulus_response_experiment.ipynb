{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294cda37",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "This notebook contains code to execute the experiments described in the paper. \n",
    "\n",
    "To reproduce some experiments you might need to change parameters such as batch size (for streaming).\n",
    "\n",
    "The largest experiments only take about 90 minutes to execute on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3248495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c586fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.stimulus_response_env import StimulusResponseEnv\n",
    "from environment.mouse_garden_env import MouseGardenEnv\n",
    "from disentangled.disentangled_agent import DisentangledAgentConfig, DisentangledAgent\n",
    "from disentangled.entangled_agent import EntangledAgentConfig, EntangledAgent\n",
    "from agent.q_learning_agent import QLearningAgent\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b395c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE_ENTANGLED = \"Entangled\"\n",
    "MODEL_TYPE_DISENTANGLED = \"Disentangled\"\n",
    "#model_type = MODEL_TYPE_ENTANGLED\n",
    "model_type = MODEL_TYPE_DISENTANGLED\n",
    "\n",
    "EXPERIMENT_TYPE_FEW_SHOT = \"Few-shot\"\n",
    "EXPERIMENT_TYPE_ZERO_SHOT = \"Zero-shot\"\n",
    "#experiment_type = EXPERIMENT_TYPE_FEW_SHOT\n",
    "experiment_type = EXPERIMENT_TYPE_ZERO_SHOT\n",
    "\n",
    "if model_type == MODEL_TYPE_DISENTANGLED:\n",
    "    TRAINING_STEPS = 6000  # disentangled\n",
    "elif model_type == MODEL_TYPE_ENTANGLED:\n",
    "    TRAINING_STEPS = 12000  # entangled\n",
    "\n",
    "EVALUATE_STEPS = 1000\n",
    "\n",
    "LOG_PERIOD = 100\n",
    "BATCH_SIZE = 16\n",
    "HISTORY_SIZE = 6\n",
    "TOKEN_EMBEDDING_SIZE = 100\n",
    "DISCOUNT = 0.9\n",
    "EPSILON_GREEDY = 0.1\n",
    "\n",
    "few_shot_training_steps = [10, 10, 20, 40, 80, 160, 320, 640, 1280]  # Cumulative: 10, 20, 40, 80, 160, ...\n",
    "\n",
    "if model_type == MODEL_TYPE_DISENTANGLED:\n",
    "    agent_config = DisentangledAgentConfig(\n",
    "\n",
    "        # BaseAgent\n",
    "        log_prefix = \"stimulus-response\",\n",
    "        log_period = LOG_PERIOD,\n",
    "        log_combined = True,\n",
    "        environment_id = MouseGardenEnv.ENV_NAME,\n",
    "        random_policy = False,  # Use to measure random policy empirically\n",
    "        print_parameters = False,\n",
    "\n",
    "        steps_training = TRAINING_STEPS,\n",
    "        steps_evaluate = EVALUATE_STEPS,\n",
    "\n",
    "        batch_size = BATCH_SIZE,\n",
    "        optimizer_learning_rate = 0.0, \n",
    "\n",
    "        # Episodic agent\n",
    "        token_embedding_size = TOKEN_EMBEDDING_SIZE,\n",
    "        history_size = HISTORY_SIZE,\n",
    "\n",
    "        # QLearningAgent\n",
    "        encoding_method = QLearningAgent.ENCODING_METHOD_RECURRENT,\n",
    "        encoded_observation_size = 200,\n",
    "        encoded_action_size = 200,\n",
    "        discount = DISCOUNT,\n",
    "        epsilon_greedy = EPSILON_GREEDY,\n",
    "\n",
    "        # Disentangled agent\n",
    "        memory_size = 10000,\n",
    "        #sparsity = 64,\n",
    "        sparsity = 32,\n",
    "        #sparsity = 16,\n",
    "        learning_rate_values = 0.1,\n",
    "    )\n",
    "\n",
    "    agent = DisentangledAgent(agent_config)\n",
    "elif model_type == MODEL_TYPE_ENTANGLED:\n",
    "    agent_config = EntangledAgentConfig(\n",
    "\n",
    "        # BaseAgent\n",
    "        log_prefix = \"stimulus-response\",\n",
    "        log_period = LOG_PERIOD,\n",
    "        log_combined = True,\n",
    "        environment_id = MouseGardenEnv.ENV_NAME,\n",
    "        random_policy = False,  # Use to measure random policy empirically\n",
    "        print_parameters = False,\n",
    "\n",
    "        steps_training = TRAINING_STEPS,\n",
    "        steps_evaluate = EVALUATE_STEPS,\n",
    "\n",
    "        batch_size = BATCH_SIZE,\n",
    "\n",
    "        # Univariate param. sweep\n",
    "        #optimizer_learning_rate = 0.01,  # Fails to learn\n",
    "        #optimizer_learning_rate = 0.001,  # Learns but can't adapt during few-shot\n",
    "        optimizer_learning_rate = 0.0001,  # Best but slow\n",
    "\n",
    "        # Episodic agent\n",
    "        token_embedding_size = TOKEN_EMBEDDING_SIZE,\n",
    "        history_size = HISTORY_SIZE,\n",
    "\n",
    "        # QLearningAgent\n",
    "        # Univariate param. sweep\n",
    "        #encoding_method = QLearningAgent.ENCODING_METHOD_RECURRENT,\n",
    "        encoding_method = QLearningAgent.ENCODING_METHOD_FLATTEN,\n",
    "        encoded_observation_size = 200,\n",
    "        encoded_action_size = 200,\n",
    "        discount = DISCOUNT,\n",
    "        epsilon_greedy = EPSILON_GREEDY,\n",
    "\n",
    "        # Disentangled agent\n",
    "        model_layers = 2,\n",
    "        model_bias = True,\n",
    "        model_nonlinearity = \"leaky-relu\",\n",
    "        model_input_layer_norm = True,\n",
    "        model_input_dropout = 0.25,\n",
    "        model_input_weight_clip = 0.0,\n",
    "\n",
    "        # Univariate param. sweep\n",
    "        #model_hidden_size = 4000, worse\n",
    "        model_hidden_size = 2000,\n",
    "        model_hidden_dropout = 0.25,\n",
    "    )\n",
    "\n",
    "    agent = EntangledAgent(agent_config)\n",
    "else:\n",
    "    raise ValueError(\"Model type not recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Few-shot experiment: \n",
    "# We train on all classes, except poisonous red foods.\n",
    "# In the few-shot phase, we stop training on the original data,\n",
    "# and only train on red foods (if we train on only poisonous red food,\n",
    "# the model will rightly learn that all red foods are poisonous).\n",
    "# We then measure performance on *all* classes to make sure it hasn't reduced.\n",
    "if experiment_type == EXPERIMENT_TYPE_FEW_SHOT:\n",
    "    print(\"Few shot experiment.\")\n",
    "    scenarios_exclude = [\"deadly nightshade\", \"fly agaric mushroom\", ]  # BAD Red food\n",
    "    scenarios_include = [\"deadly nightshade\", \"fly agaric mushroom\", \"tomato\", \"radish\"]  # ALL Red food\n",
    "\n",
    "# Zero-shot experiment\n",
    "# Remove all the dangerous birds from the training set except one.\n",
    "# We can zero-shot the test set by leveraging similar experiences with predatory birds.\n",
    "if experiment_type == EXPERIMENT_TYPE_ZERO_SHOT:\n",
    "    print(\"Zero shot experiment.\")\n",
    "    scenarios_exclude = [\"eagle\", \"falcon\"]  # Excluded from training set\n",
    "    scenarios_include = [\"eagle\", \"falcon\"]  # Included in test set\n",
    "\n",
    "def filter_scenarios(exclude_scenarios):\n",
    "    if exclude_scenarios:\n",
    "        StimulusResponseEnv.set_scenario_filter_for_envs(\n",
    "            envs = agent.envs,\n",
    "            scenario_include = None,\n",
    "            scenario_exclude = scenarios_exclude,\n",
    "        )\n",
    "    else:  # include these scenarios, and exclude everything else\n",
    "        StimulusResponseEnv.set_scenario_filter_for_envs(\n",
    "            envs = agent.envs,\n",
    "            scenario_include = scenarios_include,  # Now include ONLY these objects\n",
    "            scenario_exclude = None,\n",
    "        )\n",
    "    agent.reset()\n",
    "\n",
    "def evaluate():\n",
    "\n",
    "    # Eval on training data\n",
    "    print(\"Evaluating on training data...\")\n",
    "    filter_scenarios(exclude_scenarios=True)\n",
    "    agent.config.epsilon_greedy = 0  # Select optimal action\n",
    "    agent.do_evaluate()\n",
    "\n",
    "    # Eval on new data\n",
    "    print(\"Evaluating on new data...\")\n",
    "    filter_scenarios(exclude_scenarios=False)\n",
    "    agent.config.epsilon_greedy = 0  # Select optimal action\n",
    "    agent.do_evaluate()\n",
    "\n",
    "def training(exclude_scenarios:bool, steps:int):\n",
    "\n",
    "    # Train on new data\n",
    "    print(\"Training on new data...\")\n",
    "    filter_scenarios(exclude_scenarios)\n",
    "    agent.config.epsilon_greedy = EPSILON_GREEDY\n",
    "    agent.config.steps_training = steps\n",
    "    agent.do_training()\n",
    "\n",
    "def few_shot_training():\n",
    "    cumulative = 0\n",
    "    for steps in few_shot_training_steps:\n",
    "        cumulative += steps\n",
    "        print(f\"Few-shot training: +{steps} = {cumulative}\")\n",
    "        training(exclude_scenarios = False, steps = steps)\n",
    "        evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(exclude_scenarios = True, steps = TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_type == EXPERIMENT_TYPE_FEW_SHOT:\n",
    "    few_shot_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
